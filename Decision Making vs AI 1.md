# Should I Investigate the Impacts of Artificial Intelligence Tools and Large Language Models on Human Decision-Making Mechanisms?

The integration of Artificial Intelligence (AI) and Large Language Models (LLMs) into various aspects of modern life is rapidly transforming human activities, most notably our decision-making processes. From educational platforms employing AI-driven chatbots to sophisticated algorithms influencing managerial choices, the presence of AI is becoming increasingly pervasive. This article explores the burgeoning field investigating the impacts of these technologies on human decision-making mechanisms, drawing upon a range of academic studies to underscore the significance and multifaceted nature of this inquiry. As AI tools evolve from task-specific applications to more general and human-like intelligence, understanding their influence on our cognitive, social, and ethical landscapes becomes not just relevant but crucial. This literature review aims to illuminate the critical dimensions of this investigation, providing a robust rationale for further exploration.

The psychological impacts of AI on human decision-making are a primary area of concern and research. Studies indicate a significant relationship between AI use and a perceived 'loss of human decision-making' [1]. This perception is intertwined with the phenomenon of 'algorithm aversion,' where individuals exhibit reluctance to rely on algorithms, even after they are proven statistically more accurate than human counterparts [2, 3]. This aversion can stem from observing algorithms make errors [4], or paradoxically, from attempts to make AI appear too human-like [5].  However, research also suggests strategies to mitigate this aversion, such as demonstrating an algorithm's ability to learn and improve [6], or allowing users to slightly modify algorithmic outputs, thereby fostering a sense of control and collaboration [7].  Furthermore, the cognitive effort associated with using decision aids, including AI tools, directly impacts user satisfaction and the decision-making process [8]. Understanding the cognitive mechanisms behind adopting algorithmic advice, including factors like 'regulatory fit' [9] and the influence of experience [10], is crucial to effectively integrate AI into human workflows.  Individual differences, including personality traits and locus of control, also play a significant role in shaping trust and reliance on AI [11, 12].  Notably, psychological pressures significantly affect LLMs' decision-making capabilities, mirroring human sensitivities to stress in reasoning and social decision-making tasks [13].

Beyond individual psychology, the sociological impacts of AI and LLMs are far-reaching. AI's increasing automation capabilities are anticipated to cause social disruption and potential widespread unemployment across various sectors, exacerbating wealth inequality [14].  While AI offers positive societal impacts, particularly in healthcare, such as improved diagnostics, robotic surgery, and reduced medical errors [14], these benefits must be balanced against the risks of diminished human interaction and the erosion of face-to-face communication [14]. The spread of misinformation, potentially amplified by AI and LLMs, presents another critical societal challenge. Studies on fake news demonstrate that individuals are prone to believe information aligning with pre-existing beliefs, even if nonsensical [15], highlighting vulnerabilities in information processing that AI-driven misinformation could exploit. Conversely, interventions such as prompting users to rate the truthfulness of online content can potentially mitigate the belief in fake news [16].

Ethical considerations are paramount in the development and deployment of AI and LLMs. Concerns span privacy and data usage, bias and fairness, misinformation and manipulation, accountability and transparency [17].  The 'black box' nature of many AI systems makes it difficult to understand their decision-making processes and assign responsibility for errors, raising significant ethical dilemmas [17].  To address these, the concept of 'AI bioethics' has emerged, emphasizing principles such as beneficence, value-upholding, lucidity (transparency), and accountability [18].  These principles align with global efforts, such as the European Union's 'Ethics Guidelines for Trustworthy AI,' which stresses lawfulness, ethics, and robustness, alongside requirements for human autonomy, data privacy, explainability, and fairness [18].

Technologically, the determinants of LLM-assisted decision-making are complex and multifaceted. These determinants can be categorized into technological factors (LLM capabilities, transparency, prompt engineering), psychological factors (user's mental model, information processing, emotions), and decision-specific factors (task difficulty, accountability) [19]. Understanding the interdependencies within this framework is essential for optimizing human-AI collaboration and designing effective LLM interfaces [19].  Research into 'Explainable AI' (XAI) is crucial for enhancing transparency and user trust by visualizing AI predictions to reduce cognitive load and increase user confidence [20].  Investigating the types of decisions particularly susceptible to AI and LLM influence remains an ongoing area of exploration, although current research provides a robust foundation for such inquiries.

In conclusion, the evidence strongly suggests a significant and complex interplay between AI/LLMs and human decision-making mechanisms. From algorithm aversion to ethical considerations, the dimensions of this impact are broad and deeply intertwined with psychological, sociological, ethical, and technological factors.  The potential for both positive advancements, such as in healthcare and efficient systems, and negative consequences, like social disruption and erosion of human skills, underscores the urgent need for comprehensive investigation. Further research should focus on developing strategies for effective human-AI collaboration, mitigating algorithm aversion, ensuring ethical AI development, and understanding the long-term societal impacts.  Indeed, the question is not *if* we should investigate, but *how* deeply and broadly we must delve to navigate the evolving landscape of AI-augmented decision-making responsibly and effectively.

**References**

[1] Işık, A. İ., & Ergüner, E. (2023). Impact of artificial intelligence on human loss in decision making,laziness and safety in education. *Discover Social Science and Health*, *3*(1). (https://www.nature.com/articles/s41599-023-01787-8)

[2] Dietvorst, B. J., Simmons, J. P., & Massey, C. (2015). Algorithm aversion: people erroneously avoid algorithms after seeing them err. *Journal of Experimental Psychology: General*, *144*(1), 165. (https://doi.org/10.1037/xge0000033)

[3] Burton, J. W., Stein, M.-K., & Jensen, T. B. (2020). A systematic review of algorithm aversion in augmented decision making. *Journal of Behavioral Decision Making*, *33*(3), 476-489. (https://doi.org/10.1002/bdm.2155)

[4] Dietvorst, B. J., Simmons, J. P., & Massey, C. (2018). Overcoming algorithm aversion: people will use imperfect algorithms if they can (even slightly) modify them. *Management Science*, *64*(6), 3206-3219. (https://doi.org/10.1287%2Fmnsc.2016.2643)

[5] Hessler, P. O., Pfeiffer, J., & Hafenbradl, S. (2022). When self-humanization leads to algorithm aversion what users want from decision support systems on prosocial microlending platforms. *Electronic Markets*, *32*(3), 1619-1639. (https://link.springer.com/doi/10.1007/s12599-022-00754-y)

[6] Berger, B., Adam, M., Ruehr, A., & Benlian, A. (2021). Watch me improve—algorithm aversion and demonstrating the ability to learn. *Electronic Markets*, *31*(1), 179-197. (https://link.springer.com/doi/10.1007/s12599-020-00678-5)

[7] Sun, J., Zhang, D. J., Hu, H., & Van Mieghem, J. A. (2022). Predicting human discretion to adjust algorithmic prescription: a large-scale field experiment in warehouse operations. *Management Science*, *68*(2), 1498-1520. (https://doi.org/10.1287/mnsc.2021.3990)

[8] Bechwati, N. N., & Xia, L. (2003). Do computers sweat? The impact of perceived effort of online decision aids on consumers’ satisfaction with the decision process. *Journal of Consumer Psychology*, *13*(2), 139-153. (https://link.springer.com/doi/10.1207%2F153276603768344852)

[9] Du, X., Jia, Q., Li, F., Wang, J., & Chen, G. (2023). I will listen to you if you match with me: the effect of regulatory fit on advice taking. *Cognitive Computation*, *15*(5), 1341-1352. (https://link.springer.com/doi/10.1007/s12144-022-03571-4)

[10] Filiz, I., Judek, J. R., Lorenz, M., & Spiwoks, M. (2021). Reducing algorithm aversion through experience. *Journal of Behavioral Economics for Policy*, *5*(2), 83-91. (https://doi.org/10.1016/j.jbef.2021.100524)

[11] Sharan, N. N., & Romano, D. M. (2020). The effects of personality and locus of control on trust in humans versus artificial intelligence. *Heliyon*, *6*(7), e04572. (https://doi.org/10.1016/j.heliyon.2020.e04572)

[12] McBride, M., Carter, L., & Ntuen, C. (2012). The impact of personality on nurses’ bias towards automated decision aid acceptance. *International Journal of Information Systems and Cognitive Science*, *1*(1), 55-74. (https://doi.org/10.1504/IJISCM.2012.051148)

[13] Li, J., Lyu, P., & Zhang, H. (2024). Will LLMs Sink or Swim? Exploring Decision-Making Under Pressure. *Findings of the Association for Computational Linguistics: EMNLP 2024*. (https://aclanthology.org/2024.findings-emnlp.668.pdf)

[14] Tae-Young, J. (2020). The impact of artificial intelligence on human society and bioethics. *Tzu Chi Medical Journal*, *32*(4), 339. (https://pmc.ncbi.nlm.nih.gov/articles/PMC7605294/)

[15] Moravec, P. L., Minas, R. K., & Dennis, A. R. (2019). Fake news on social media: people believe what they want to believe when it makes no sense at all. *MIS Quarterly*, *43*(4). (https://doi.org/10.25300/misq/2019/15505)

[16] Moravec, P. L., Kim, A., Dennis, A. R., & Minas, R. K. (2022). Do you really know if it’s true? How Asking users to rate stories affects belief in fake news on social media. *Information Systems Research*, *33*(4), 1591-1610. (https://doi.org/10.1287/isre.2021.1090)

[17] Cave, S., & Craig, C. S. (2023). The Ethical Implications of Large Language Models in AI. *IEEE Computer Society*. (https://www.computer.org/publications/tech-news/trends/ethics-of-large-language-models-in-ai/)

[18] Tae-Young, J. (2020). The impact of artificial intelligence on human society and bioethics. *Tzu Chi Medical Journal*, *32*(4), 339. (https://pmc.ncbi.nlm.nih.gov/articles/PMC7605294/pdf/TCMJ-32-339.pdf)

[19] Gu, B., & Wang, W. Y. C. (2024). Determinants of LLM-assisted Decision-Making. *arXiv preprint arXiv:2402.17385*. (https://arxiv.org/html/2402.17385v1)

[20] Hudon, A., Demazure, T., Karran, A., Léger, P.-M., & Sénécal, S. (2021). Explainable Artificial Intelligence (XAI): how the visualization of AI predictions affects user cognitive load and confidence. In *Augmented Intelligence* (pp. 234-243). Springer International Publishing. (https://link.springer.com/doi/10.1007/978-3-030-88900-5_27)